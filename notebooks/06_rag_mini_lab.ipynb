{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§ª Advanced: RAG (Retrieval Augmented Generation) Mini-Lab\n",
    "\n",
    "## Goal\n",
    "Teach an agent to answer questions about a *private* document (a text file) that it wasn't trained on. This is the basis of 'Chat with your PDF' apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-generativeai chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a Dummy Knowledge Base\n",
    "We'll make a simple text file with some 'secret' info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secret_text = \"\"\"\n",
    "PROJECT OMEGA: \n",
    "The secret launch date is December 25th, 2025.\n",
    "The code word is 'Blueberry'.\n",
    "The team lead is Dr. Smith.\n",
    "\"\"\"\n",
    "\n",
    "# In a real app, you'd load this from a PDF or txt file\n",
    "documents = [secret_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Simple Vector Search (Conceptual)\n",
    "Instead of a full database for this mini-lab, we'll simulate the RAG process:\n",
    "1. **Retrieve:** Find relevant text.\n",
    "2. **Augment:** Stuff it into the prompt.\n",
    "3. **Generate:** Ask the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_KEY_HERE\"\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "model = genai.GenerativeModel('gemini-flash-lite-latest')\n",
    "\n",
    "def rag_chat(user_query):\n",
    "    # Step 1: Retrieve (Simplified - we just grab our one doc)\n",
    "    context = documents[0]\n",
    "    \n",
    "    # Step 2: Augment\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful assistant. Answer the user's question using ONLY the context provided below.\n",
    "    \n",
    "    CONTEXT:\n",
    "    {context}\n",
    "    \n",
    "    QUESTION:\n",
    "    {user_query}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 3: Generate\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "# Let's ask about the secret info\n",
    "print(rag_chat(\"When is the launch date?\"))\n",
    "print(rag_chat(\"What is the code word?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Why this matters\n",
    "Without the context, the model wouldn't know about 'Project Omega'. Try asking it directly without the context function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content(\"What is the code word for Project Omega?\")\n",
    "print(\"Standard Model Response:\", response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
